{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c66d86b",
   "metadata": {},
   "source": [
    "## Using Text Blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdabfe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity of Text 1 is 1.0\n",
      "Polarity of Text 2 is 0.0\n",
      "Subjectivity of Text 1 is 1.0\n",
      "Subjectivity of Text 2 is 0.0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text_1 = \"The movie was so awesome.\"\n",
    "text_2 = \"The food here tastes terrible.\"\n",
    "text_4 = \"Its an fine okayish phone\"\n",
    "text_3 = \"This was a helpful example but I would prefer another one\"\n",
    "\n",
    "#Determining the Polarity \n",
    "p_1 = TextBlob(text_1).sentiment.polarity\n",
    "p_3 = TextBlob(text_3).sentiment.polarity\n",
    "\n",
    "#Determining the Subjectivity\n",
    "s_1 = TextBlob(text_1).sentiment.subjectivity\n",
    "s_3 = TextBlob(text_3).sentiment.subjectivity\n",
    "\n",
    "print(\"Polarity of Text 1 is\", p_1)\n",
    "print(\"Polarity of Text 2 is\", p_3)\n",
    "print(\"Subjectivity of Text 1 is\", s_1)\n",
    "print(\"Subjectivity of Text 2 is\", s_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8206b81",
   "metadata": {},
   "source": [
    "## Using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f51bb5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of text 1: {'neg': 0.0, 'neu': 0.73, 'pos': 0.27, 'compound': 0.5719}\n",
      "Sentiment of text 2: {'neg': 0.508, 'neu': 0.492, 'pos': 0.0, 'compound': -0.4767}\n",
      "Sentiment of text 3: {'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.2263}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "text_1 = \"The book was a perfect balance between wrtiting style and plot.\"\n",
    "text_2 =  \"The pizza tastes terrible.\"\n",
    "text_3 = \"Its an fine okayish phone\"\n",
    "text_3 = \"This was a helpful example but I would prefer another one\"\n",
    "sent_1 = sentiment.polarity_scores(text_1)\n",
    "sent_2 = sentiment.polarity_scores(text_2)\n",
    "sent_3 = sentiment.polarity_scores(text_3)\n",
    "print(\"Sentiment of text 1:\", sent_1)\n",
    "print(\"Sentiment of text 2:\", sent_2)\n",
    "print(\"Sentiment of text 3:\", sent_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68994e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5671aac4",
   "metadata": {},
   "source": [
    "## Using Bag of Words Vectorization-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b24bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Loading the Dataset\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv('Finance_data.csv')\n",
    "# #Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "# cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "# text_counts = cv.fit_transform(data['sentences'])\n",
    "# #Splitting the data into trainig and testing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['feedback'], test_size=0.25, random_state=5)\n",
    "# #Training the model\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# MNB = MultinomialNB()\n",
    "# MNB.fit(X_train, Y_train)\n",
    "# #Caluclating the accuracy score of the model\n",
    "# from sklearn import metrics\n",
    "# predicted = MNB.predict(X_test)\n",
    "# accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "# print(\"Accuracuy Score: \",accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a7c2d",
   "metadata": {},
   "source": [
    "## Using LSTM-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Loading the dataset\n",
    "data = pd.read_csv('Finance_data.csv')\n",
    "#Pre-Processing the text \n",
    "def cleaning(df, stop_words):\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "    # Replacing the digits/numbers\n",
    "    df['sentences'] = df['sentences'].str.replace('d', '')\n",
    "    # Removing stop words\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
    "    # Lemmatization\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "    return df\n",
    "stop_words = stopwords.words('english')\n",
    "data_cleaned = cleaning(data, stop_words)\n",
    "#Generating Embeddings using tokenizer\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(data_cleaned['verified_reviews'].values)\n",
    "X = tokenizer.texts_to_sequences(data_cleaned['verified_reviews'].values)\n",
    "X = pad_sequences(X)\n",
    "#Model Building\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(352, activation='LeakyReLU'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#Model Training\n",
    "model.fit(X_train, y_train, epochs = 20, batch_size=32, verbose =1)\n",
    "#Model Testing\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00aa824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b73cc56",
   "metadata": {},
   "source": [
    "## Using Transformer-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f698f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f9a3599b9a4aa0a93d241b539d97c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392659dc1dc4485bdab0cb3fc1717a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)\"tf_model.h5\";:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5c99f073a4126a9382d8111f78278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aa92062ce84ae5b14f3b4c5c2aeabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9994569420814514},\n",
       " {'label': 'NEGATIVE', 'score': 0.9987302422523499}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"It was the best of times.\", \"t was the worst of times.\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c1a564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9987401366233826}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"Bitcoin should be transparent and it isn‚Äôt as easy as people thinks it is, there are so many strategies to be learnt and unfolded about Bitcoin trading\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ba198f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997841715812683},\n",
       " {'label': 'NEGATIVE', 'score': 0.9415776133537292},\n",
       " {'label': 'POSITIVE', 'score': 0.999841570854187}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\" back ground music waste.\",\" Unlable hear properly.\", \"good speech.\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6eb852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9972597360610962}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"If she's asking for it, she can just ask for it\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d5c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7037141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.391, 'pos': 0.609, 'compound': 0.8313}\n",
      "Sentiment of text 1: {'neg': 0.0, 'neu': 0.575, 'pos': 0.425, 'compound': 0.5719}\n",
      "Sentiment of text 2: {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}\n",
      "Sentiment of text 3: {'neg': 0.0, 'neu': 0.526, 'pos': 0.474, 'compound': 0.2023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create an instance of the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to clean the text and tokenize it\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Define a function to perform sentiment analysis on a given text\n",
    "def analyze_sentiment(text):\n",
    "    # Preprocess the text\n",
    "    text = preprocess_text(text)\n",
    "    # Analyze the sentiment using the SentimentIntensityAnalyzer\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "text = \"I love driving my new car! It's so much fun.\"\n",
    "sentiment = analyze_sentiment(text)\n",
    "print(sentiment)\n",
    "\n",
    "text_1 = \"The book was a perfect balance between wrtiting style and plot.\"\n",
    "text_2 =  \"The pizza tastes terrible.\"\n",
    "text_3 = \"Its an fine okayish phone\"\n",
    "sent_1 = analyze_sentiment(text_1)\n",
    "sent_2 = analyze_sentiment(text_2)\n",
    "sent_3 = analyze_sentiment(text_3)\n",
    "print(\"Sentiment of text 1:\", sent_1)\n",
    "print(\"Sentiment of text 2:\", sent_2)\n",
    "print(\"Sentiment of text 3:\", sent_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0965a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plus is awesome but Apple should have at least included 90Hz refresh rate. It would have improved the sales.\n",
      "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.802}\n",
      "\n",
      "It amazes me the fact that despite having same os, same battery and a less energy consuming display this phone battery endurance is significantly worse compared to same size battery iPhone 13/14 max. Basically Apple has done this intentionally, there is no other reason.\n",
      "{'neg': 0.103, 'neu': 0.731, 'pos': 0.166, 'compound': 0.228}\n",
      "\n",
      "Love this phone 3 days of having and the battery last long ! I had the 11pro for 3 years it‚Äôs was all messed up and cracked so I had to get another phone ! I was going to get the pro max but they was out of stock I‚Äôm impatient and just wanted a fresh screen !\n",
      "{'neg': 0.153, 'neu': 0.6, 'pos': 0.246, 'compound': 0.5826}\n",
      "\n",
      "This phone covers a niche audience where they want a vanilla 14 in a bigger form factor without the need for the Pro features. This is a better attempt than the Mini phones because more and more people prioritize battery life and bigger screens.\n",
      "{'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.4939}\n",
      "\n",
      "I have this phone and it‚Äôs a great great phone n the gen 8 watch has much more options paired with the 14 that the 13 don‚Äôt have n iOS 16 is strong\n",
      "{'neg': 0.0, 'neu': 0.489, 'pos': 0.511, 'compound': 0.91}\n",
      "\n",
      "The iPhone 14 has improved audio out from the lightning socket for wired IEMs (I have lighting wired IEMs). Do you know if the iPhone 14 plus has the same improved audio out conaore to the iPhone 13?\n",
      "{'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'compound': 0.7351}\n",
      "\n",
      "Love it, love iPhones in general but I will never buy one until they finally put the darn USB Type-C on it. Every gadget I own uses the Type-C port. I can&#39;t keep a separate thing just for the phone. Frustrates me too much just the thought of it.\n",
      "{'neg': 0.087, 'neu': 0.661, 'pos': 0.252, 'compound': 0.7579}\n",
      "\n",
      "The iPhone 14 Plus is even lighter than the iPhone XS Max! ü§Ø\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "It‚Äôs the exact same as the 13 pro max minus 1 camera and the 120hz screen. I would get which ever one is cheaper. I have the 14pro max. I shouldn‚Äôt have upgraded from the 13pro max not worth it. The 13pro max is a beast.\n",
      "{'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.2263}\n",
      "\n",
      "I have never bought pro models in my life but this time when i upgrade from xr, freaking apple forced me to get a pro model by not updating processor. Anyways personally I don&#39;t like pro because it&#39;s heavy in hand, some say it&#39;s good and premium feel but I just love how light regular Iphone 14 is compared to pro.\n",
      "{'neg': 0.119, 'neu': 0.617, 'pos': 0.263, 'compound': 0.7906}\n",
      "\n",
      "It&#39;s not a bad phone but paying literally flagship prices for a 60Hz display and last year&#39;s chipset is not wise. Go for the Pro Max if you want something big, whether 13 or 14\n",
      "{'neg': 0.124, 'neu': 0.671, 'pos': 0.205, 'compound': 0.0772}\n",
      "\n",
      "Basically if you want the 14 plus go with the 13 pro max,nearly same price plus 120 hertz and longer battery life (according to gsm arena basically) edit: forgot to mention also the 13 pro has a better wide angle might not make a difference in daylight but in night you can tell and plus telephoto and better optimized\n",
      "{'neg': 0.0, 'neu': 0.776, 'pos': 0.224, 'compound': 0.8442}\n",
      "\n",
      "Apple needs to offer at least 90hz for &quot;budget&quot; iphones\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Amazing review, very eye-opening\n",
      "{'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'compound': 0.5859}\n",
      "\n",
      "Niceeeeeeee Review Sir! I don&#39;t know the feeling of having an iphone so watching your review amaze me! ‚ò∫Ô∏è‚ò∫Ô∏è‚ò∫Ô∏è\n",
      "{'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'compound': 0.6792}\n",
      "\n",
      "Will looks like a deadlift machine üëçüí™\n",
      "{'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.3612}\n",
      "\n",
      "The only reson i might b getting this is its cinematic mode that can do 4k 24fps.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "In my country theres 400dollar difference between 14+ and 14pro max so yeah its even lighter..\n",
      "{'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compound': 0.296}\n",
      "\n",
      "I wanted the iPhone 13 Pro Max, but apple have discontinued it so I can‚Äôt finance it. And I don‚Äôt do locked sim contracts (more money) so it was just a better choice for me to get 14 plus financially as other places it‚Äôs more expensive that the plus. ¬£48 a month for iPhone 14 plus, ¬£52 a month for iPhone 13 Pro Max\n",
      "{'neg': 0.0, 'neu': 0.914, 'pos': 0.086, 'compound': 0.4404}\n",
      "\n",
      "With a premium price only get a 60hz refresh rate. What a bummer\n",
      "{'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'compound': -0.3818}\n",
      "\n",
      "The conclusion is, it depends on your preference and usage. Why spend more on pro version if you doesn&#39;t use or need the feature ?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "if only i could get my hands on modern technology for once üò¢üò¢.my phone‚Äôs broken and i don‚Äôt think i can get a new one especially at this time.unless help comes from somewhere üò¢üò¢üò¢\n",
      "{'neg': 0.14, 'neu': 0.725, 'pos': 0.135, 'compound': -0.0314}\n",
      "\n",
      "I found this thing and it worked forreal i got one. Its in the name.. Real simple to do too!\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "6Ohz is not so good but IOS animations are buttery smooth.\n",
      "{'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'compound': 0.4404}\n",
      "\n",
      "There are 2 things people should know in life. <br>Number 1. Each year the newest iPhone pro model is the best iPhone in the family but it is not the best phone in the phone market. <br>Number 2. The human body adjust itself automatically. This is something a lot of people don&#39;t understand. Is there better tech available in the tech world yes but it is a subjective yes. Your body once it has become used to a certain set of parameters will feel uncomfortable once it leave that set of parameters but it can adjust itself with time. What does that mean? It mean you can become used to higher refresh rate and higher resolution but if you go back to lower refresh rate or lower resolution it doesn&#39;t mean you won&#39;t be able to get used to it again. The body will just adjust itself to get used to a new set of parameters again. This is something marketing and businessmen won&#39;t tell you. You have to know these by yourself and then learn how to use and master a product fully. <br><br>You can chase the best techs of course but you have to also understand it isn&#39;t necessary if you don&#39;t want it to be.\n",
      "{'neg': 0.066, 'neu': 0.719, 'pos': 0.215, 'compound': 0.9565}\n",
      "\n",
      "Tbh if you see someone with this and other vanilla variant, honestly you wouldn&#39;t know if they had this or the 13. This hasn&#39;t and will not get the wow factor\n",
      "{'neg': 0.0, 'neu': 0.618, 'pos': 0.382, 'compound': 0.7783}\n",
      "\n",
      "I don&#39;t think anybody in their right mind is going to  pay  $900 for this phone. I predict this is the first and the last year we&#39;ll see this model, if budget and value for money is the issue, try and find a 13 Promax new or get the    new Pixel.\n",
      "{'neg': 0.047, 'neu': 0.872, 'pos': 0.081, 'compound': 0.25}\n",
      "\n",
      "i need to go for this because 13 pro max is out of stock at my place. :&#39;(\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "I predict this is the first and the last year we&#39;ll see this model, if budget and value for money is the issue, try and find a 13 Promax new or get the    new Pixel.\n",
      "{'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.34}\n",
      "\n",
      "Y‚Äôall goofy af in the comments lol. Everyone doesn‚Äôt want the same phone. For some people the 14 plus is a great choice.\n",
      "{'neg': 0.0, 'neu': 0.523, 'pos': 0.477, 'compound': 0.802}\n",
      "\n",
      "How can you guys not bothered by the annoying notch?\n",
      "{'neg': 0.714, 'neu': 0.286, 'pos': 0.0, 'compound': -0.6124}\n",
      "\n",
      "The iPhone 14 plus is basically the same as a iPhone 13 Pro Max. Idk why Apple did this. If you want the true iPhone 14 experience go with the 14 pro or pro max. Better chipset, camera and dynamic island. The A16 is a monster. It‚Äôs undefeated and will be for years.\n",
      "{'neg': 0.04, 'neu': 0.686, 'pos': 0.274, 'compound': 0.802}\n",
      "\n",
      "We have an issue selling 14 plus 1260 Euro üí∂ There are customers walking in the store look at the phone and as for 199 Euro price some had the value 99 Euro phones and they said : It&#39;s a child iPhone none like it none buy it .....They mostly buy 145 Euro Realmi Narzo 50a\n",
      "{'neg': 0.058, 'neu': 0.876, 'pos': 0.066, 'compound': 0.0747}\n",
      "\n",
      "iPhone 14 Plus is a gaming/media watching phone.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Before y‚Äôall bash this phone, try it out first, then be the judge and not go by what someone else say\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "is it dual sim?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "The front camera has terrible auto focus problem. <br><a href=\"https://www.youtube.com/watch?v=HDkd_nO1tvA&amp;t=6m27s\">6:27</a>\n",
      "{'neg': 0.279, 'neu': 0.721, 'pos': 0.0, 'compound': -0.7003}\n",
      "\n",
      "best phone in 14 series\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n",
      "\n",
      "I always wonder what&#39;s the title of that song in your speaker test.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Only Apple Knows What They Are Doing...\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Budget had zero to do with my decision‚Ä¶ the store had the 14 plus in stock and not the other.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Was showing us the motorbike guy who fell really necessary üòÇ\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "When is the Pixel 7 review coming?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "only gsmarena that does proper battery tests\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "This phone may sell. Here me out if you pay for this threw bill crediting. Thier lowering the monthly cost to 5 bucks. Ofc you have to get an unlimited plan for it. Unlimited starts at 65 or 75 depending on carrier. But it works well. Plus when you add coverage that like 17 bucks. So monthly bill will come to 88 bucks.\n",
      "{'neg': 0.086, 'neu': 0.758, 'pos': 0.157, 'compound': 0.4215}\n",
      "\n",
      "This is a joke played intentionally by Apple so that the majority is forced to choose the Pro models. It would have been better if they had not released these models at all that have recycled parts from last year&#39;s phones.\n",
      "{'neg': 0.118, 'neu': 0.588, 'pos': 0.294, 'compound': 0.5423}\n",
      "\n",
      "Thank you to Review this phone it&#39;s good Camera\n",
      "{'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.6597}\n",
      "\n",
      "Still rocking my 12 pro max\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "<a href=\"https://www.youtube.com/watch?v=HDkd_nO1tvA&amp;t=0m08s\">0:08</a> I‚Äôm pretty sure if someone can afford the 14 Plus they can afford the 14 Pro Max too\n",
      "{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'compound': 0.6705}\n",
      "\n",
      "Should people worry about software update? cause of last year chip\n",
      "{'neg': 0.293, 'neu': 0.707, 'pos': 0.0, 'compound': -0.4404}\n",
      "\n",
      "Basically it&#39;s a big iPhone 13\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Waiting for the google pixel 7 series review üò¨\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Better this or 12pm?\n",
      "{'neg': 0.0, 'neu': 0.256, 'pos': 0.744, 'compound': 0.4404}\n",
      "\n",
      "iPhone 14 is $899? The pixel 7 pro is $899 lol. Which one is a better purchase?\n",
      "{'neg': 0.0, 'neu': 0.569, 'pos': 0.431, 'compound': 0.7236}\n",
      "\n",
      "Ip 14+ or ip 13 pro? Pls help me choose.\n",
      "{'neg': 0.0, 'neu': 0.6, 'pos': 0.4, 'compound': 0.4588}\n",
      "\n",
      "Its really just a bigger version of the iPhone 13. The iPhone 13 <a href=\"http://plus.haha.lol/\">plus.haha.lol</a>. Because same everything even same A15 chip. Only difference is screen size. Why couldn&#39;t apple atleast give us the A16 chip and 9x zoom video?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "I&#39;m confused with all the different iphone 14 names. üòï\n",
      "{'neg': 0.315, 'neu': 0.685, 'pos': 0.0, 'compound': -0.3182}\n",
      "\n",
      "I actually prefer this to the 14 pro max\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "The battery life isn‚Äôt good? Whuuuut? üß¢\n",
      "{'neg': 0.0, 'neu': 0.479, 'pos': 0.521, 'compound': 0.504}\n",
      "\n",
      "14, 14plus=Electronic waste. 14 pro üëçüëçüëç\n",
      "{'neg': 0.359, 'neu': 0.641, 'pos': 0.0, 'compound': -0.4215}\n",
      "\n",
      "Im happy with my 14 plus. Upgraded from the xr, i dont care about 120hz\n",
      "{'neg': 0.197, 'neu': 0.525, 'pos': 0.278, 'compound': 0.2668}\n",
      "\n",
      "Apple fanboys will still buy this model. Thats why it was made\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "This fon doesn&#39;t make sense to me even I was shopping for a newer iPhone I would consider this one .i would consider 13 Pro max for the same money üí∞\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Dude i love the shirt üò≥<br>Where we can get it?\n",
      "{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.6369}\n",
      "\n",
      "I wanted to buy 14 plus but I got the 13 pro max ‚Ä¶ it‚Äôs so much better\n",
      "{'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.4404}\n",
      "\n",
      "ü§©üëç\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "why do you need 2 seperate videos for 14 and 14 plus? Ain&#39;t it the same, except the size- which means more battery only? Such a waste of time\n",
      "{'neg': 0.208, 'neu': 0.792, 'pos': 0.0, 'compound': -0.4871}\n",
      "\n",
      "Larger phones are great but not iPhones and that&#39;s because of their aspect ratio. They&#39;re way too wide and really uncomfortable for one handed usage. I wish they made narrower phones. Until then, I&#39;ma stick to my 6.1&quot; iPhones.\n",
      "{'neg': 0.094, 'neu': 0.684, 'pos': 0.222, 'compound': 0.6003}\n",
      "\n",
      "üòç\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Nice üëç from Pakistan\n",
      "{'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'compound': 0.4215}\n",
      "\n",
      "It has 512 gb storage so its okay for me\n",
      "{'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.2263}\n",
      "\n",
      "‚ù§Ô∏è\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "now do Goolge P 7 Vs Sam  A73 5G And iPhone 14.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Old A15 bionic High price 60HZ\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "I went from an iphone 12 to an iphone 14 plus, guess i should have just gotten an iphone 13 pro max judging from the comments\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Apple fooling continues...\n",
      "{'neg': 0.474, 'neu': 0.526, 'pos': 0.0, 'compound': -0.4019}\n",
      "\n",
      "still iphone 14 pro much much better\n",
      "{'neg': 0.0, 'neu': 0.674, 'pos': 0.326, 'compound': 0.4404}\n",
      "\n",
      "13 Pro Max is much better than this one.\n",
      "{'neg': 0.0, 'neu': 0.633, 'pos': 0.367, 'compound': 0.4404}\n",
      "\n",
      "Wow never thought id find content lke this! <a href=\"https://www.youtube.com/watch?v=HDkd_nO1tvA&amp;t=1m03s\">1:03</a>\n",
      "{'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.6239}\n",
      "\n",
      "The heck is a vanilla model?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "records in 60 fps uploads 30fps video lol\n",
      "{'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.4215}\n",
      "\n",
      "In Iran the price are too ridiculous iPhone 14 is 35 billion Ô∑º <br>14 plus is 45 billion Ô∑º<br>14 pro is 62 billion Ô∑º<br>14 pro max is 68 billion Ô∑º<br>But the older 13 series is too expensive than 14 because they can registered but the newer models can‚Äôt because they‚Äôre don‚Äôt gonna buy iPhone from apple and bring it in Iran anymore üòÇ it‚Äôs ridiculous because it‚Äôs just from apple in USA they don‚Äôt let people buy the new iPhones because it has sos satellite üò¢ and 13 series price is <br>iPhone 13 is 53 billion Ô∑º<br>13 mini is 48 billion Ô∑º<br>13 pro is 170 billion Ô∑º <br>13 pro max is 210 billion Ô∑º üò±<br>You can buy a car with 210 billion Ô∑º in Iran but it has same price as 13 pro max üòÇ what can I do? Is it too funny that I live in Iran ? HELPME BRING ME TO HEEL Instead of Being in IRANüò¢\n",
      "{'neg': 0.055, 'neu': 0.915, 'pos': 0.03, 'compound': -0.3527}\n",
      "\n",
      "are you kidding me?, how would you notice the difference of 60hz and 120hz in just scrolling with small screen? lmao, you should play first person shooter with higher fps with fast phase movements or games that produce higher fps.. can your mobile phone with 120hz produce 120fps?, even 30hz will be smooth in just scrolling üòÇ\n",
      "{'neg': 0.0, 'neu': 0.785, 'pos': 0.215, 'compound': 0.8042}\n",
      "\n",
      "ü§©ü§©ü§©ü§©ü§©ü§©ü§©ü§©ü§©ü§©ü§©ü§©üëèüèª\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Where is your pixel?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Tbh i miss angieüíú\n",
      "{'neg': 0.444, 'neu': 0.556, 'pos': 0.0, 'compound': -0.1531}\n",
      "\n",
      "Get the 13promax instead\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "60hz is dead, I&#39;ve been using 240hz monitor and 120hz phone for years so 60hz makes me sick\n",
      "{'neg': 0.432, 'neu': 0.568, 'pos': 0.0, 'compound': -0.8225}\n",
      "\n",
      "üëçüèΩ<br>227\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "BRING BACK    -----------------------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  &quot;A N G I E &quot; <b>***********</b>\n",
      "{'neg': 0.0, 'neu': 0.409, 'pos': 0.591, 'compound': 0.9524}\n",
      "\n",
      "Why don&#39;t you get your own channel?\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Or wait till iPhone 15\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Wag mo kami lokohin ohma tokita\n",
      "{'neg': 0.194, 'neu': 0.806, 'pos': 0.0, 'compound': -0.0516}\n",
      "\n",
      "Nc\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Just got my 14 Plus and I already regret it. My girlfriends 12 Pro Max runs better than it. I just upgraded from an IPhone XR and it literally just feels like a larger version of it. There is 0 fucking reason this phone shouldn‚Äôt have the same refresh rate.\n",
      "{'neg': 0.096, 'neu': 0.719, 'pos': 0.185, 'compound': 0.3818}\n",
      "\n",
      "Just buy an iphone 13\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "i am tired of them showing the same pictures in every phone review they do and doing the same song aswell. unsubcribed\n",
      "{'neg': 0.266, 'neu': 0.734, 'pos': 0.0, 'compound': -0.4404}\n",
      "\n",
      "1th ü§ü\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Always 2\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Awful phone\n",
      "{'neg': 0.75, 'neu': 0.25, 'pos': 0.0, 'compound': -0.4588}\n",
      "\n",
      "Okay\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.2263}\n",
      "\n",
      "First\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "For people who only want to buy an Iphone and nothing else <br>It would be actually better to buy 13pro or 13 pro max over this crap\n",
      "{'neg': 0.131, 'neu': 0.657, 'pos': 0.212, 'compound': 0.1531}\n",
      "\n",
      "1st comment\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "6th\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "60hz for THAT price? Ew\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Meanwhile.  I‚Äôm still too broke to have a battery replacement for my iPhone 8 Plus. ü•≤\n",
      "{'neg': 0.318, 'neu': 0.682, 'pos': 0.0, 'compound': -0.4215}\n",
      "\n",
      "To be honest for it&#39;s price, it&#39;s a very bad phone\n",
      "{'neg': 0.324, 'neu': 0.37, 'pos': 0.306, 'compound': -0.0516}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#define an empty dictionary\n",
    "l = []\n",
    "\n",
    "# load the pickle contents\n",
    "with open ('comments1.pkl','rb') as pick:\n",
    "    l.append(pickle.load(pick))\n",
    "# print(l)\n",
    "for i in l[0]:\n",
    "    print(i[0], end='\\n')\n",
    "    print(analyze_sentiment(i[0]), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2d641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2281300f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument should be integer or bytes-like object, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11588/4250240369.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# search for the data you want to scrape, for example the user's followers count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"edge_followed_by\\\":{\\\"count\\\":\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"},\\\"followed_by_viewer\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfollowers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument should be integer or bytes-like object, not 'str'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# make a GET request to the Instagram page\n",
    "response = requests.get(\"https://www.instagram.com/rvcjinsta\")\n",
    "\n",
    "# check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # extract the content of the page\n",
    "    content = response.content\n",
    "    \n",
    "    # search for the data you want to scrape, for example the user's followers count\n",
    "    start = content.find(\"\\\"edge_followed_by\\\":{\\\"count\\\":\") + 27\n",
    "    end = content.find(\"},\\\"followed_by_viewer\\\"\")\n",
    "    followers = content[start:end]\n",
    "    \n",
    "    # print the followers count\n",
    "    print(\"Followers:\", followers)\n",
    "else:\n",
    "    print(\"Request failed with status code\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc40a120",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.instagram.com', port=443): Max retries exceeded with url: /rvcjindia (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000211AA9484F0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000211AA9484F0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.instagram.com', port=443): Max retries exceeded with url: /rvcjindia (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000211AA9484F0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/1364905548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# make a GET request to the Instagram page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.instagram.com/rvcjindia\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# check if the request was successful\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPSConnectionPool(host='www.instagram.com', port=443): Max retries exceeded with url: /rvcjindia (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000211AA9484F0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# make a GET request to the Instagram page\n",
    "response = requests.get(\"https://www.instagram.com/rvcjindia\")\n",
    "\n",
    "# check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # extract the content of the page\n",
    "    content = response.content\n",
    "    \n",
    "    # search for the data you want to scrape, for example the user's followers count\n",
    "    start = content.find(\"\\\"edge_followed_by\\\":{\\\"count\\\":\") + 27\n",
    "    end = content.find(\"},\\\"followed_by_viewer\\\"\")\n",
    "    followers = content[start:end]\n",
    "    \n",
    "    # print the followers count\n",
    "    print(\"Followers:\", followers)\n",
    "else:\n",
    "    print(\"Request failed with status code\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553b2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a586a2c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.6.1-py3-none-any.whl (188 kB)\n",
      "Collecting prawcore<3,>=2.1\n",
      "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting update-checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting websocket-client>=0.54.0\n",
      "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\vansh\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vansh\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vansh\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vansh\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vansh\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2)\n",
      "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
      "Successfully installed praw-7.6.1 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9119a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3039119c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py:32: RuntimeWarning: coroutine 'Twint.main' was never awaited\n",
      "  from .rate_limit import RateLimiter\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\vansh\\anaconda3\\lib\\site-packages\\praw\\models\\reddit\\draft.py:7: RuntimeWarning: coroutine 'Twint.main' was never awaited\n",
      "  from .subreddit import Subreddit\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\vansh\\anaconda3\\lib\\site-packages\\websocket\\_app.py:11: RuntimeWarning: coroutine 'Twint.main' was never awaited\n",
      "  from ._core import WebSocket, getdefaulttimeout\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display Name: redditdev\n"
     ]
    },
    {
     "ename": "RequestException",
     "evalue": "error with request Invalid return character or leading space in header: User-Agent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidHeader\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             return self._http.request(\n\u001b[0m\u001b[0;32m     59\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    527\u001b[0m         )\n\u001b[1;32m--> 528\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    457\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_headers\u001b[1;34m(self, headers)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;31m# Raise exception on invalid header value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m                 \u001b[0mcheck_header_validity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\utils.py\u001b[0m in \u001b[0;36mcheck_header_validity\u001b[1;34m(header)\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid return character or leading space in header: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidHeader\u001b[0m: Invalid return character or leading space in header: User-Agent",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRequestException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/2522550178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Display the title of the Subreddit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Title:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Display the description of the Subreddit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;34m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\models\\reddit\\subreddit.py\u001b[0m in \u001b[0;36m_fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\models\\reddit\\subreddit.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\util\\deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 )\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_old_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At most one of 'data' or 'json' is supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m             return self._core.request(\n\u001b[0m\u001b[0;32m    942\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"api_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         return self._request_with_retries(\n\u001b[0m\u001b[0;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    183\u001b[0m     ):\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             response = self._rate_limiter.call(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headers\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_set_header_callback\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"refresh\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ):\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Authorization\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"bearer {self._authorizer.access_token}\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0madditional_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scope\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mgrant_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://oauth.reddit.com/grants/installed_client\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         self._request_token(\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[0mgrant_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrant_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mdevice_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_device_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36m_request_token\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    153\u001b[0m         )\n\u001b[0;32m    154\u001b[0m         \u001b[0mpre_request_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Why are these OKAY responses?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36m_post\u001b[1;34m(self, url, success_status, **data)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccess_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ok\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         response = self._requestor.request(\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\prawcore\\requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m             )\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRequestException\u001b[0m: error with request Invalid return character or leading space in header: User-Agent"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit_read_only = praw.Reddit(client_id=\"\",\t\t # your client id\n",
    "\t\t\t\t\t\t\tclient_secret=\"\",\t # your client secret\n",
    "\t\t\t\t\t\t\tuser_agent=\"\")\t # your user agent\n",
    "\n",
    "\n",
    "subreddit = reddit_read_only.subreddit(\"redditdev\")\n",
    "\n",
    "# Display the name of the Subreddit\n",
    "print(\"Display Name:\", subreddit.display_name)\n",
    "\n",
    "# Display the title of the Subreddit\n",
    "print(\"Title:\", subreddit.title)\n",
    "\n",
    "# Display the description of the Subreddit\n",
    "print(\"Description:\", subreddit.description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99cdd716",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/3337082665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"taylor_swift_tweets.csv\"\u001b[0m     \u001b[1;31m# path to csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtwint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mSearch\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProfile_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPandas_au\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autoget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\twint\\run.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTwint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mFavorites\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \"\"\"\n\u001b[0;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import twint\n",
    "\n",
    "c = twint.Config()\n",
    "\n",
    "c.Search = ['Taylor Swift']       # topic\n",
    "c.Limit = 500      # number of Tweets to scrape\n",
    "c.Store_csv = True       # store tweets in a csv file\n",
    "c.Output = \"taylor_swift_tweets.csv\"     # path to csv file\n",
    "\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4624ef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\anaconda3\\lib\\abc.py:102: RuntimeWarning: coroutine 'Twint.main' was never awaited\n",
      "  return _abc_subclasscheck(cls, subclass)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'taylor_swift_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/3690869237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taylor_swift_tweets.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'taylor_swift_tweets.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('taylor_swift_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45af0b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/1086150313.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a75c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a868acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9684ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vansh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75ddc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1129ffd2",
   "metadata": {},
   "source": [
    "NNP: Noun, proper, singular\n",
    "NN: Noun, common, singular or mass\n",
    "IN: Preposition or conjunction, subordinating\n",
    "VBG: Verb, gerund or present participle\n",
    "VBN: Verb, past participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ee7d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "515aad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "563807f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa5ddfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51d4157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84ab1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89bd0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecbe66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "be156d57",
   "metadata": {},
   "source": [
    "nlp_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c74aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "Accuracy is: 0.9923333333333333\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    992.6 : 1.0\n",
      "                 welcome = True           Positi : Negati =     36.7 : 1.0\n",
      "                follower = True           Positi : Negati =     35.4 : 1.0\n",
      "                     sad = True           Negati : Positi =     30.0 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.8 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.8 : 1.0\n",
      "                     idk = True           Negati : Positi =     13.8 : 1.0\n",
      "                congrats = True           Positi : Negati =     12.9 : 1.0\n",
      "                    glad = True           Positi : Negati =     12.5 : 1.0\n",
      "                   let's = True           Positi : Negati =     12.2 : 1.0\n",
      "None\n",
      "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "\n",
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9e12b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963333333333333"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9963333333333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d41190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n",
      "5000\n",
      "5000\n",
      "20000\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput:\\n\\n#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\\n@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\\n@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\\n@97sides CONGRATS :)\\nyeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())\n",
    "'''\n",
    "Output:\n",
    "\n",
    "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n",
    "'''\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(pos_tweets)) # Output: 5000\n",
    "\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(neg_tweets)) # Output: 5000\n",
    "\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (len(all_tweets)) # Output: 20000\n",
    "\n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet)\n",
    "'''\n",
    "Output:\n",
    "\n",
    "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
    "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
    "@97sides CONGRATS :)\n",
    "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f67e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88dc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', 'good', 'morn']\n",
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "['one', 'irresist', 'flipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []   \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))\n",
    "'''\n",
    "Output:\n",
    "\n",
    "['hello', 'great', 'day', 'good', 'morning']\n",
    "'''\n",
    "\n",
    "print (pos_tweets[5])\n",
    "'''\n",
    "Output:\n",
    "\n",
    "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
    "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
    "'''\n",
    "\n",
    "print (clean_tweets(pos_tweets[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cdba986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': True, 'great': True, 'day': True, 'good': True, 'morn': True}\n",
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words) \n",
    "    return words_dictionary\n",
    "\n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "print (bag_of_words(custom_tweet))\n",
    "\n",
    "# '''\n",
    "# Output:\n",
    "# {'great': True, 'good': True, 'morning': True, 'hello': True, 'day': True}\n",
    "# '''\n",
    "\n",
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos')) \n",
    "\n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "\n",
    "print (len(pos_tweets_set), len(neg_tweets_set)) # Output: (5000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8991644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    "\n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    "\n",
    "print(len(test_set),  len(train_set)) # Output: (2000, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29c6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723\n",
      "Most Informative Features\n",
      "                     sad = True              neg : pos    =     40.2 : 1.0\n",
      "                     via = True              pos : neg    =     36.3 : 1.0\n",
      "                     bam = True              pos : neg    =     25.0 : 1.0\n",
      "                     x15 = True              neg : pos    =     19.0 : 1.0\n",
      "                  welcom = True              pos : neg    =     16.7 : 1.0\n",
      "                   arriv = True              pos : neg    =     15.0 : 1.0\n",
      "                   didnt = True              neg : pos    =     15.0 : 1.0\n",
      "                    glad = True              pos : neg    =     14.2 : 1.0\n",
      "               goodnight = True              pos : neg    =     13.0 : 1.0\n",
      "                    poor = True              neg : pos    =     12.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    "\n",
    "print (classifier.show_most_informative_features(10))   \n",
    "# '''\n",
    "# Output:\n",
    "# 0.723\n",
    "# Most Informative Features\n",
    "#                      via = True              pos : neg    =     37.0 : 1.0\n",
    "#                     glad = True              pos : neg    =     25.0 : 1.0\n",
    "#                      sad = True              neg : pos    =     22.6 : 1.0\n",
    "#                       aw = True              neg : pos    =     21.7 : 1.0\n",
    "#                      bam = True              pos : neg    =     21.0 : 1.0\n",
    "#                      x15 = True              neg : pos    =     19.7 : 1.0\n",
    "#                  appreci = True              pos : neg    =     17.7 : 1.0\n",
    "#                    arriv = True              pos : neg    =     15.0 : 1.0\n",
    "#                      ugh = True              neg : pos    =     14.3 : 1.0\n",
    "#                   justin = True              neg : pos    =     13.0 : 1.0\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca51f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.931815829423131\n",
      "0.06818417057687025\n",
      "pos\n",
      "<ProbDist with 2 samples>\n",
      "pos\n",
      "0.0003673552891362668\n",
      "0.999632644710862\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (classifier.classify(custom_tweet_set)) # Output: neg\n",
    "# Negative tweet correctly classified as negative\n",
    "\n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (prob_result.max()) # Output: neg\n",
    "print (prob_result.prob(\"neg\")) # Output: 0.941844352481\n",
    "print (prob_result.prob(\"pos\")) # Output: 0.0581556475194\n",
    "\n",
    "\n",
    "custom_tweet = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "\n",
    "print (classifier.classify(custom_tweet_set)) # Output: pos\n",
    "# Positive tweet correctly classified as positive\n",
    "\n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (prob_result.max()) # Output: pos\n",
    "print (prob_result.prob(\"neg\")) # Output: 0.00131055449755\n",
    "print (prob_result.prob(\"pos\")) # Output: 0.998689445502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06a30d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.7095864661654135\n",
      "pos recall: 0.755\n",
      "pos F-measure: 0.7315891472868217\n",
      "\n",
      "neg precision: 0.7382478632478633\n",
      "neg recall: 0.691\n",
      "neg F-measure: 0.7138429752066116\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    "\n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    "\n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    "\n",
    "    predicted_label = classifier.classify(feature)\n",
    "\n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    "\n",
    "print('pos precision:', precision(actual_set['pos'], predicted_set['pos'])) # Output: pos precision: 0.762896825397\n",
    "print('pos recall:', recall(actual_set['pos'], predicted_set['pos'])) # Output: pos recall: 0.769\n",
    "print('pos F-measure:', f_measure(actual_set['pos'], predicted_set['pos'])) # Output: pos F-measure: 0.76593625498\n",
    "print()\n",
    "print('neg precision:', precision(actual_set['neg'], predicted_set['neg'])) # Output: neg precision: 0.767137096774\n",
    "print('neg recall:', recall(actual_set['neg'], predicted_set['neg'])) # Output: neg recall: 0.761\n",
    "print('neg F-measure:', f_measure(actual_set['neg'], predicted_set['neg'])) # Output: neg F-measure: 0.7640562249"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
